{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    'I love programming',\n",
        "    'Programming is fun',\n",
        "    'Deep learning is interesting',\n",
        "    'Natural language processing is exciting'\n",
        "]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uXYjkwNdOdr",
        "outputId": "4eefa5e4-1eae-479c-d7fe-5388ba2a046c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the mapping of indices to words\n",
        "index_to_word = tokenizer.index_word\n",
        "\n",
        "# Print the actual words for the all indices\n",
        "for index in range(0, total_words+1):\n",
        "    word = index_to_word.get(index, 'Unknown')\n",
        "    print(f\"Index {index}: Word '{word}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCGrOCi20-9_",
        "outputId": "5a2751ea-ad63-4ce8-d830-5695dc704657"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 0: Word 'Unknown'\n",
            "Index 1: Word 'is'\n",
            "Index 2: Word 'programming'\n",
            "Index 3: Word 'i'\n",
            "Index 4: Word 'love'\n",
            "Index 5: Word 'fun'\n",
            "Index 6: Word 'deep'\n",
            "Index 7: Word 'learning'\n",
            "Index 8: Word 'interesting'\n",
            "Index 9: Word 'natural'\n",
            "Index 10: Word 'language'\n",
            "Index 11: Word 'processing'\n",
            "Index 12: Word 'exciting'\n",
            "Index 13: Word 'Unknown'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences and labels\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-9LCWMdzrIM",
        "outputId": "99fce399-60a4-47fc-94a8-a5431f6778ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 4],\n",
              " [3, 4, 2],\n",
              " [2, 1],\n",
              " [2, 1, 5],\n",
              " [6, 7],\n",
              " [6, 7, 1],\n",
              " [6, 7, 1, 8],\n",
              " [9, 10],\n",
              " [9, 10, 11],\n",
              " [9, 10, 11, 1],\n",
              " [9, 10, 11, 1, 12]]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences for equal length\n",
        "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNxu1uDgzrJz",
        "outputId": "b10a98ca-91c9-4718-a8f7-c1edf34864ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  3,  4],\n",
              "       [ 0,  0,  3,  4,  2],\n",
              "       [ 0,  0,  0,  2,  1],\n",
              "       [ 0,  0,  2,  1,  5],\n",
              "       [ 0,  0,  0,  6,  7],\n",
              "       [ 0,  0,  6,  7,  1],\n",
              "       [ 0,  6,  7,  1,  8],\n",
              "       [ 0,  0,  0,  9, 10],\n",
              "       [ 0,  0,  9, 10, 11],\n",
              "       [ 0,  9, 10, 11,  1],\n",
              "       [ 9, 10, 11,  1, 12]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGg-BMnbDkvW",
        "outputId": "19cd3174-5d30-4c56-bdbd-a76a542a666e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 2, 1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create input features and labels\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "print(X[2], y[2])\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "print(X[2], y[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV7VffYx1rNr",
        "outputId": "cddf4350-d350-42af-951e-0088f578cbff"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 2] 1\n",
            "[0 0 0 2] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' PARAMETERS OF EMBEDDING() LAYER FUNCTION\n",
        "total_words: The first parameter specifies the size of the vocabulary, i.e., the total number of unique words in your corpus.\n",
        "In this case, it is calculated as len(tokenizer.word_index) + 1, where tokenizer.word_index contains the mapping of words to\n",
        "indices.\n",
        "50 (vector size): The second parameter is the dimensionality of the word vectors. In this case, it is set to 50. Each word\n",
        "in the vocabulary will be represented as a vector of length 50 in the embedding space.\n",
        "input_length=max_sequence_length-1: The third parameter is the length of input sequences that the embedding layer will receive.\n",
        " It should match the length of your input sequences, which is set to max_sequence_length-1 to exclude the last word, which is\n",
        " the target word.\n",
        "'''"
      ],
      "metadata": {
        "id": "wsdSjUSOdRaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple LSTM model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(total_words, 50, input_length=max_sequence_length-1),\n",
        "    tf.keras.layers.LSTM(100),\n",
        "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQBFDydqz0Ao",
        "outputId": "681490e8-28c3-434a-a8f5-71d52b4b8c33"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.5671 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.5603 - accuracy: 0.0909\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.5535 - accuracy: 0.3636\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.5467 - accuracy: 0.2727\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.5398 - accuracy: 0.2727\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.5327 - accuracy: 0.2727\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.5253 - accuracy: 0.2727\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.5176 - accuracy: 0.2727\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.5095 - accuracy: 0.2727\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.5009 - accuracy: 0.2727\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.4917 - accuracy: 0.2727\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.4818 - accuracy: 0.2727\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.4711 - accuracy: 0.2727\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.4595 - accuracy: 0.2727\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.4468 - accuracy: 0.2727\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.4331 - accuracy: 0.2727\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.4181 - accuracy: 0.2727\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4016 - accuracy: 0.2727\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.3837 - accuracy: 0.2727\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.3641 - accuracy: 0.2727\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.3427 - accuracy: 0.2727\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.3194 - accuracy: 0.2727\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2943 - accuracy: 0.2727\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2674 - accuracy: 0.2727\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2390 - accuracy: 0.2727\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.2096 - accuracy: 0.2727\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1799 - accuracy: 0.2727\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1513 - accuracy: 0.2727\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1256 - accuracy: 0.2727\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.1048 - accuracy: 0.2727\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.0902 - accuracy: 0.2727\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0807 - accuracy: 0.2727\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.0724 - accuracy: 0.2727\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.0609 - accuracy: 0.2727\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.0441 - accuracy: 0.2727\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.0228 - accuracy: 0.2727\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9991 - accuracy: 0.2727\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9753 - accuracy: 0.2727\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.9528 - accuracy: 0.2727\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.9321 - accuracy: 0.2727\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.9129 - accuracy: 0.2727\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8943 - accuracy: 0.2727\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8756 - accuracy: 0.2727\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.8560 - accuracy: 0.2727\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.8349 - accuracy: 0.2727\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.8120 - accuracy: 0.2727\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7871 - accuracy: 0.2727\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.7603 - accuracy: 0.2727\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7316 - accuracy: 0.2727\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.7014 - accuracy: 0.2727\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.6700 - accuracy: 0.2727\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.6376 - accuracy: 0.2727\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6045 - accuracy: 0.3636\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.5705 - accuracy: 0.3636\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5351 - accuracy: 0.3636\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 1.4978 - accuracy: 0.4545\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.4582 - accuracy: 0.4545\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.4162 - accuracy: 0.4545\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3722 - accuracy: 0.4545\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3266 - accuracy: 0.6364\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2801 - accuracy: 0.6364\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2328 - accuracy: 0.6364\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1848 - accuracy: 0.6364\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1360 - accuracy: 0.7273\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0867 - accuracy: 0.7273\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0373 - accuracy: 0.8182\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9886 - accuracy: 0.8182\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9413 - accuracy: 0.9091\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.8957 - accuracy: 0.9091\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8516 - accuracy: 0.9091\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.8088 - accuracy: 0.9091\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7674 - accuracy: 0.9091\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7276 - accuracy: 0.9091\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6894 - accuracy: 0.9091\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6527 - accuracy: 0.9091\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6169 - accuracy: 0.9091\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5824 - accuracy: 0.9091\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5497 - accuracy: 0.9091\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5187 - accuracy: 0.9091\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4892 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4612 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4350 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4103 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3868 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3647 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3440 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3244 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3059 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2886 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2722 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2567 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2423 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2286 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2156 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2034 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1917 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1806 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1702 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1603 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1511 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78cd1afa99f0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have trained your model and named it 'model'\n",
        "\n",
        "# Get the weights of the embedding layer\n",
        "embedding_weights = model.layers[0].get_weights()[0]  # Assuming Embedding layer is the first layer\n",
        "\n",
        "# Get the vocabulary size and embedding dimension\n",
        "vocab_size, embedding_dim = embedding_weights.shape\n",
        "\n",
        "# Create a dictionary to map words to their corresponding vectors\n",
        "word_vectors = {}\n",
        "word_index = tokenizer.word_index  # Assuming you have a tokenizer for your text data\n",
        "for word, index in word_index.items():\n",
        "    # Skip the padding token if present\n",
        "    if index == 0:\n",
        "        continue\n",
        "    # Get the embedding vector for the word\n",
        "    embedding_vector = embedding_weights[index]\n",
        "    # Store the word vector in the dictionary\n",
        "    word_vectors[word] = embedding_vector\n",
        "\n",
        "# Now you can access the word vectors using the word as key\n",
        "print(word_vectors['love'])  # Replace 'word' with the actual word you want to get the vector for\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvzwbTyuERMf",
        "outputId": "22a9ddae-7309-4079-a669-c4998fc04049"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.09916745  0.14381759  0.10741768 -0.06482245 -0.14453943  0.05845503\n",
            "  0.11049834 -0.08802968 -0.09631979  0.13130462  0.05166156  0.10901786\n",
            " -0.13262478  0.16100314  0.07237987 -0.00364961 -0.1476928   0.03410875\n",
            " -0.0566395   0.05281119  0.1128921  -0.09121816 -0.10135016 -0.01110511\n",
            " -0.03484494  0.15506019  0.10438553 -0.06440771 -0.12294479  0.13778603\n",
            " -0.14966315  0.08684783  0.150789   -0.0264182   0.13509698 -0.03096593\n",
            "  0.11994029  0.10678666 -0.14893937  0.11432906  0.12786056 -0.09751885\n",
            " -0.1112318  -0.08798535 -0.06698811 -0.1169666   0.09707578 -0.11540285\n",
            "  0.04978655 -0.14818183]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ykolr7KE6hb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}